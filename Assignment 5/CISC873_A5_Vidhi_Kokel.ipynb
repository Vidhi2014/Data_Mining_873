{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZdMuySPvud1"
   },
   "source": [
    "### Author: Vidhi Kokel\n",
    "# Understand the template\n",
    "\n",
    "#### What is the experimental protocol used and how was it carried out? How did we tune hyper-parameters in the template? What is the search space and what is the criteria to determine good/bad hyper-parameters?\n",
    "#### ‚úÖThe experimental protocols used here are a combination of following layers combined in a sequential model with multiple layers. \n",
    "1. Embedding Layer\n",
    "2. LSTM Layer\n",
    "3. GRU Layer\n",
    "4. Bidirectional LSTM Layer\n",
    "5. Birdirectional GRU Layer\n",
    "6. Convolutional Layer\n",
    "7. Max Pooling Layer\n",
    "8. Fully Connected Layer\n",
    "\n",
    "#### The search space for the experiment is various hyper-parameters that can affect the performance of the model which are listed below.\n",
    "1. Number of Hidden layers and units\n",
    "2. Different types of layers\n",
    "3. Weight Initialization\n",
    "4. Activation functions\n",
    "5. Learning Rate\n",
    "6. Number of epochs\n",
    "7. Batch size\n",
    "\n",
    "#### The template shows how a multi-modality (both text and image inputs) multi-objective (predicting both price and type) solution can be provided for the current problem by using Embedding and reduced_mean layers for text inputs and a convolutional layer with max pooling for the image inputs. The loss weights have been set in the template. The criteria to decide good/bad hyper-parameters depends on how well the neural network is able to learn. It should not under-fit or overfit.\n",
    "\n",
    "# Problem Definition\n",
    "#### Define the problem. What is the input? What is the output? What data mining function is required? What could be the challenges? What is the impact? What is an ideal solution?\n",
    "#### ‚úÖWe are given the textual summary and images of multiple air bnb listings in Montreal in 2019. From these summaries and images we have to classify the type of the listing as well as the price category in which the property belongs to. There are following 3 sets of inputs used accross all the experiments. \n",
    "\n",
    "- Text(summary) + Image\n",
    "- Text(summary) only\n",
    "- Image only\n",
    "\n",
    "#### Moreover, there are following 3 sets of outputs representing the type and price category of the property listing that are possible from the above experiments. But since we have to find out the price in our problem, I have only used price prediction for submitting the solutions on Kaggle.\n",
    "\n",
    "- Price + Type\n",
    "- Price only\n",
    "- Type only\n",
    "\n",
    "#### Classification is required for this problem. The challenges could be the image resolution, unclear summary and not clean textual summary. The impact might be misclassification of the types and prices leading to false positives or false negatives. The ideal solution is a classification algorithm that accurately identifies the price and type categories for a given listing.\n",
    "\n",
    "# Theoretical Questions\n",
    "\n",
    "#### üåàIs fully-connected model a good one for sequential data? Why? How about for image data? Is it good? Why?\n",
    "#### ‚úÖFor sequential data fully-connected model is a good model\n",
    "#### Because they are ‚Äústructure agnostic.‚Äù That is, no special assumptions need to be made about the inputs.\n",
    "#### ‚úÖFor image data fully-connected model is not a good model.\n",
    "#### Because when it comes to classifying images ‚Äî lets say with size 64x64x3 ‚Äî fully connected layers need 12288 weights in the first hidden layer! The number of weights will be even bigger for images with size 225x225x3 = 151875. Networks having large number of parameter face several problems, for e.g. slower training time, chances of overfitting e.t.c.\n",
    "##### Source: https://medium.datadriveninvestor.com/convolution-neural-networks-vs-fully-connected-neural-networks-8171a6e86f15\n",
    "#### üåàWhat is gradient vanishing and gradient explosion, and how GRU/LSTM tries to mitigate this problem?\n",
    "#### ‚úÖGradient Vanishing\n",
    "#### Vanishing Gradient occurs when the derivative or slope will get smaller and smaller as we go backward with every layer during backpropagation.\n",
    "#### When weights update is very small or exponential small, the training time takes too much longer, and in the worst case, this may completely stop the neural network training.\n",
    "#### A vanishing Gradient problem occurs with the sigmoid and tanh activation function because the derivatives of the sigmoid and tanh activation functions are between 0 to 0.25 and 0‚Äì1. Therefore, the updated weight values are small, and the new weight values are very similar to the old weight values. This leads to Vanishing Gradient problem. We can avoid this problem using the ReLU activation function because the gradient is 0 for negatives and zero input, and 1 for positive input.\n",
    "#### ‚úÖGradient explosion\n",
    "#### Exploding gradient occurs when the derivatives or slope will get larger and larger as we go backward with every layer during backpropagation. This situation is the exact opposite of the vanishing gradients.\n",
    "#### This problem happens because of weights, not because of the activation function. Due to high weight values, the derivatives will also higher so that the new weight varies a lot to the older weight, and the gradient will never converge. So it may result in oscillating around minima and never come to a global minima point.\n",
    "##### Source: https://medium.datadriveninvestor.com/vanishing-and-exploding-gradients-in-neural-networks-bddd4504e59c \n",
    "#### ‚úÖHow GRU/LSTM tries to mitigate this problem\n",
    "#### LSTMs solve the problem using a unique additive gradient structure that includes direct access to the forget gate‚Äôs activations, enabling the network to encourage desired behaviour from the error gradient using frequent gates update on every time step of the learning process.\n",
    "##### Source: https://medium.datadriveninvestor.com/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577#:~:text=LSTMs%20solve%20the%20problem%20using,step%20of%20the%20learning%20process.\n",
    "#### GRU‚Äôs are able to solve the vanishing gradient problem by using an update gate and a reset gate. The update gate controls information that flows into memory, and the reset gate controls the information that flows out of memory. The update gate and reset gate are two vectors that decide which information will get passed on to the output. They can be trained to keep information from the past or remove information that is irrelevant to the prediction.\n",
    "##### Source: https://deepai.org/machine-learning-glossary-and-terms/gated-recurrent-unit\n",
    "#### üåàWhat is multi-objective/multi-task learning? What is multi-modality learning? How do you use them in this assignment?\n",
    "#### ‚úÖMulti-objective/multi-task learning: The learning task where multiple objectives are achieved or multiple tasks are performed parallely is called multi-objective/multi-task learning. For eg. Here, in this problem we are trying to predict the price and type categories of the listing thus achieveing results for both of them parallely.\n",
    "#### ‚úÖMulti-modality learning: The learning task where 2 or more than 2 types/kinds of inputs are used is called multi-modality learning. For eg. Here, in this problem we are using both listing summaries and thumbnails as inputs to predict their respective type and price categories.\n",
    "\n",
    "#### üåàWhat is the difference among xgboost, lightgbm and catboost?\n",
    "#### ‚úÖTree Symmetry\n",
    "#### In CatBoost, symmetric trees, or balanced trees, refer to the splitting condition being consistent across all nodes at the same depth of the tree. LightGBM and XGBoost, on the other hand, results in asymmetric trees, meaning splitting condition for each node across the same depth can differ. Even though LightGBM and XGBoost are both asymmetric trees, LightGBM grows leaf-wise (horizontally) while XGBoost grows level-wise (vertically). To put it simply, we can think of LightGBM as growing the tree selectively, resulting in smaller and faster models compared to XGBoost.\n",
    "#### ‚úÖSplitting Method\n",
    "#### In CatBoost, a greedy method is used such that a list of possible candidates of feature-split pairs are assigned to the leaf as the split and the split that results in the smallest penalty is selected.\n",
    "#### In LightGBM, Gradient-based One-Side Sampling (GOSS) keeps all data instances with large gradients and performs random sampling for data instances with small gradients.\n",
    "#### In XGBoost, the pre-sorted algorithm considers all feature and sorts them by feature value. After which, a linear scan is done to decide the best split for the feature and feature value that results in the most information gain.\n",
    "#### ‚úÖType of Boosting\n",
    "#### CatBoost supports ordered boosting while others don't. Ordered boosting refers to the case when each model trains on a subset of data and evaluates another subset of data. Benefits of ordered boosting include increasing robustness to unseen data.\n",
    "#### ‚úÖData-type Support\n",
    "- Numerical Columns: Supported by all 3.\n",
    "- Categorical Columns: Supported by all 3 but with some constraints of encoding the categorical values.\n",
    "- Text Columns: Only supported by CatBoost.\n",
    "\n",
    "#### ‚úÖHandle Missing Values\n",
    "#### All of them handle the missing values\n",
    "#### ‚úÖPerformance\n",
    "#### Generally, from the literature, XGBoost and LightGBM yield similar performance, with CatBoost and LightGBM performing much faster than XGBoost, especially for larger datasets.\n",
    "##### Source: https://towardsdatascience.com/catboost-vs-lightgbm-vs-xgboost-c80f40662924\n",
    "\n",
    "# Trial Discussion\n",
    "\n",
    "#### ‚úÖ‚úÖ‚úÖ‚úÖHere from all the 5 approaches, multi modality implementations outperform the implementations using only text inputs and that using only text inputs outperform the implementation that uses only image inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KiMrkOjeWj1A"
   },
   "outputs": [],
   "source": [
    "# you can also download the data by running the following line (linux only) \n",
    "# if you already got the data from kaggle, you can skip this cell.\n",
    "\n",
    "# ! wget https://github.com/CISC-873/Information-2021/releases/download/data/a4.zip\n",
    "# ! unzip -q a4.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "BEur8cLCR1Zk",
    "outputId": "d53ea915-7517-4251-e4ab-f0a58ba504d4"
   },
   "outputs": [],
   "source": [
    "# Import the required libraries and datasets\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "xy_train_df = pd.read_csv('train_xy.csv')\n",
    "x_test_df = pd.read_csv('test_x.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>image</th>\n",
       "      <th>type</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spacious, sunny and cozy modern apartment in t...</td>\n",
       "      <td>img_train/0.jpg</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Located in one of the most vibrant and accessi...</td>\n",
       "      <td>img_train/1.jpg</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logement coquet et douillet √† 10 minutes du ce...</td>\n",
       "      <td>img_train/2.jpg</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Beautiful and spacious (1076 sc ft, / 100 mc) ...</td>\n",
       "      <td>img_train/3.jpg</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tr√®s grand appartement ''rustique'' et tr√®s ag...</td>\n",
       "      <td>img_train/4.jpg</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7622</th>\n",
       "      <td>Un grand logement 4 et 1/2, tout inclut, bien ...</td>\n",
       "      <td>img_train/7626.jpg</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7623</th>\n",
       "      <td>Magnificent condo directly on the river. You w...</td>\n",
       "      <td>img_train/7627.jpg</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7624</th>\n",
       "      <td>This apartment is perfect for anyone visiting ...</td>\n",
       "      <td>img_train/7628.jpg</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7625</th>\n",
       "      <td>It is a cozy ,clean ,and comfortable apartment...</td>\n",
       "      <td>img_train/7629.jpg</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7626</th>\n",
       "      <td>Modern country style (newly-renovated); open c...</td>\n",
       "      <td>img_train/7630.jpg</td>\n",
       "      <td>House</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7627 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                summary               image  \\\n",
       "0     Spacious, sunny and cozy modern apartment in t...     img_train/0.jpg   \n",
       "1     Located in one of the most vibrant and accessi...     img_train/1.jpg   \n",
       "2     Logement coquet et douillet √† 10 minutes du ce...     img_train/2.jpg   \n",
       "3     Beautiful and spacious (1076 sc ft, / 100 mc) ...     img_train/3.jpg   \n",
       "4     Tr√®s grand appartement ''rustique'' et tr√®s ag...     img_train/4.jpg   \n",
       "...                                                 ...                 ...   \n",
       "7622  Un grand logement 4 et 1/2, tout inclut, bien ...  img_train/7626.jpg   \n",
       "7623  Magnificent condo directly on the river. You w...  img_train/7627.jpg   \n",
       "7624  This apartment is perfect for anyone visiting ...  img_train/7628.jpg   \n",
       "7625  It is a cozy ,clean ,and comfortable apartment...  img_train/7629.jpg   \n",
       "7626  Modern country style (newly-renovated); open c...  img_train/7630.jpg   \n",
       "\n",
       "           type  price  \n",
       "0     Apartment      1  \n",
       "1     Apartment      0  \n",
       "2     Apartment      1  \n",
       "3     Apartment      1  \n",
       "4     Apartment      0  \n",
       "...         ...    ...  \n",
       "7622  Apartment      0  \n",
       "7623  Apartment      2  \n",
       "7624  Apartment      1  \n",
       "7625  Apartment      0  \n",
       "7626      House      1  \n",
       "\n",
       "[7627 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the training dataframe\n",
    "xy_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>summary</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Charming warm house is ready to host you here ...</td>\n",
       "      <td>img_test/0.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>La chambre est spacieuse et lumineuse, dans un...</td>\n",
       "      <td>img_test/1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Grande chambre confortable situ√©e au sous-sol ...</td>\n",
       "      <td>img_test/2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Pr√®s d‚Äôun M√©tro, ligne orange. 10 minutes √† pi...</td>\n",
       "      <td>img_test/3.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Very bright appartment and very cosy. 2 separa...</td>\n",
       "      <td>img_test/4.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7355</th>\n",
       "      <td>7626</td>\n",
       "      <td>Large, fully-furnished flat with brick walls a...</td>\n",
       "      <td>img_test/7627.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7356</th>\n",
       "      <td>7627</td>\n",
       "      <td>Logement situ√© dans le haut d‚Äôun duplex. Vivez...</td>\n",
       "      <td>img_test/7628.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7357</th>\n",
       "      <td>7628</td>\n",
       "      <td>My place is close to parks, . My place is good...</td>\n",
       "      <td>img_test/7629.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7358</th>\n",
       "      <td>7629</td>\n",
       "      <td>*** For security reasons, I will prioritize gu...</td>\n",
       "      <td>img_test/7630.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7359</th>\n",
       "      <td>7630</td>\n",
       "      <td>Stay in an amazing area of Montreal! 5-7 min f...</td>\n",
       "      <td>img_test/7631.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7360 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                            summary  \\\n",
       "0        0  Charming warm house is ready to host you here ...   \n",
       "1        1  La chambre est spacieuse et lumineuse, dans un...   \n",
       "2        2  Grande chambre confortable situ√©e au sous-sol ...   \n",
       "3        3  Pr√®s d‚Äôun M√©tro, ligne orange. 10 minutes √† pi...   \n",
       "4        4  Very bright appartment and very cosy. 2 separa...   \n",
       "...    ...                                                ...   \n",
       "7355  7626  Large, fully-furnished flat with brick walls a...   \n",
       "7356  7627  Logement situ√© dans le haut d‚Äôun duplex. Vivez...   \n",
       "7357  7628  My place is close to parks, . My place is good...   \n",
       "7358  7629  *** For security reasons, I will prioritize gu...   \n",
       "7359  7630  Stay in an amazing area of Montreal! 5-7 min f...   \n",
       "\n",
       "                  image  \n",
       "0        img_test/0.jpg  \n",
       "1        img_test/1.jpg  \n",
       "2        img_test/2.jpg  \n",
       "3        img_test/3.jpg  \n",
       "4        img_test/4.jpg  \n",
       "...                 ...  \n",
       "7355  img_test/7627.jpg  \n",
       "7356  img_test/7628.jpg  \n",
       "7357  img_test/7629.jpg  \n",
       "7358  img_test/7630.jpg  \n",
       "7359  img_test/7631.jpg  \n",
       "\n",
       "[7360 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the testing dataframe\n",
    "x_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "11e60d895e9142beab189eaf0fa0df14",
      "41e453fda22e4508afa28b421569e561",
      "6e11ee69f84d420a938e196b07ba6274",
      "195c3403a30f4350a69abdc1caafa795",
      "148d0078a22543a395909db21f947d4d",
      "05e4f892aa7043299e8675bf2329eb2c",
      "8354b303c3a14eec8ab2d81bd8a9da30",
      "4aacc03bbd0b433a8ff3fbd8f1a72231",
      "13780d4de6ab4b038f1b6f021bae1a35"
     ]
    },
    "id": "7NGojpH_R1Zl",
    "outputId": "41fa16d4-a93d-44ae-ae41-1a7cba61dde7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f1b7e837ad48e889b0adcc79b998e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocess image data\n",
    "\n",
    "def load_image(file):\n",
    "    try:\n",
    "        image = Image.open(\n",
    "            file\n",
    "        ).convert('LA').resize((64, 64))\n",
    "        arr = np.array(image)\n",
    "    except:\n",
    "        arr = np.zeros((64, 64, 2))\n",
    "    return arr\n",
    "\n",
    "\n",
    "# loading images:\n",
    "x_image = np.array([load_image(i) for i in tqdm(xy_train_df.image)])\n",
    "\n",
    "# loading summary: (force convert some of the non-string cell to string)\n",
    "x_text = xy_train_df.summary.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "Ayqfe7rQS0Mk",
    "outputId": "a7018cda-3079-4cff-f472-25ffe1e129aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x277abe4c2e0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check image loading\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_image[0, :, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1z3FEfYsR1Zm",
    "outputId": "5cb7fa2b-bdf7-478c-a6df-71db64117859"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique values for price category 3 [1 0 2]\n",
      "unique values for type category 24 [ 1 17 22 10 18 20  5  2  8  4 23 13 15 16 14 11 19  0 21  3  6 12  7  9]\n",
      "(6101, 64, 64, 2)\n",
      "(1526, 64, 64, 2)\n",
      "(6101,)\n",
      "(1526,)\n",
      "(6101,)\n",
      "(1526,)\n"
     ]
    }
   ],
   "source": [
    "# Split training dataset and labels into training and validation datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# labels:\n",
    "y_price = xy_train_df.price\n",
    "y_type = xy_train_df.type.astype('category').cat.codes\n",
    "\n",
    "len_price = len(y_price.unique())\n",
    "len_type = len(y_type.unique())\n",
    "print('unique values for price category', len_price, y_price.unique())\n",
    "print('unique values for type category', len_type, y_type.unique())\n",
    "\n",
    "# splitting:\n",
    "\n",
    "x_tr_image, x_vl_image, x_tr_text, x_vl_text, y_tr_price, y_vl_price, y_tr_type, y_vl_type = train_test_split(\n",
    "    x_image, \n",
    "    x_text,\n",
    "    y_price,\n",
    "    y_type,\n",
    "    test_size=0.2)\n",
    "\n",
    "print(np.shape(x_tr_image))\n",
    "print(np.shape(x_vl_image))\n",
    "print(np.shape(y_tr_price))\n",
    "print(np.shape(y_vl_price))\n",
    "print(np.shape(y_tr_type))\n",
    "print(np.shape(y_vl_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xh70MnCDR1Zm",
    "outputId": "78b82e49-0bf3-4a35-b848-bfef4bfadaff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vidhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vidhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6101, 100)\n",
      "(1526, 100)\n"
     ]
    }
   ],
   "source": [
    "# preprocess text data\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pprint import pprint\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "  \n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "vocab_size = 40000\n",
    "# vocab_size = 10000\n",
    "max_len = 100\n",
    "\n",
    "\n",
    "# build vocabulary from training set\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(x_tr_text)\n",
    "\n",
    "# clean and standardize the summaries\n",
    "def clean_text(list_of_text):\n",
    "    cleaned_text = []\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for text in list_of_text:\n",
    "        text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\n",
    "        text = text.lower()\n",
    "        text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n",
    "        text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n",
    "        text = [word for word in text if  not word in stop_words]\n",
    "        text = \" \".join(text)\n",
    "        cleaned_text.append(text)\n",
    "    return cleaned_text\n",
    "\n",
    "def _preprocess(list_of_text):\n",
    "    return pad_sequences(\n",
    "        tokenizer.texts_to_sequences(list_of_text),\n",
    "        maxlen=max_len,\n",
    "        padding='post',\n",
    "    )\n",
    "    \n",
    "# Clean text\n",
    "# x_tr_text_cleaned = clean_text(x_tr_text)\n",
    "# x_vl_text_cleaned = clean_text(x_vl_text)\n",
    "    \n",
    "# padding is done inside: \n",
    "# x_tr_text_id = _preprocess(x_tr_text_cleaned)\n",
    "# x_vl_text_id = _preprocess(x_vl_text_cleaned)\n",
    "x_tr_text_id = _preprocess(x_tr_text)\n",
    "x_vl_text_id = _preprocess(x_vl_text)\n",
    "\n",
    "print(x_tr_text_id.shape)\n",
    "print(x_vl_text_id.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p5E5zYSiR1Zm",
    "outputId": "251efb34-8937-4def-9447-e3fe8c845fd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['appartement propre et calme dans un beau duplexe de 3 appartements seulement '\n",
      " 'pour 2 personnes et il y a un canap√© dans le salon pour une troisi√®me '\n",
      " 'personne 2 balcons devant au salon et derri√®re dans la cuisine qui donne sur '\n",
      " 'un beau jardin pour admirer la nature durant le pti dej chauffage efficace '\n",
      " 'et conviviable universit√© de montr√©al et hec sont √† 15min de marche √† 2 min '\n",
      " 'de marche le bus 165 peut vous mener partout via m√©tro des supermarch√©s et '\n",
      " 'restaurations sont juste √† cot√©',\n",
      " 'fully furnished and fully equipped 2 floors house in montreal it is a 2600 '\n",
      " 'sq foot house that can accommodate several people i can add more mattresses '\n",
      " 'if needed for large groups this house is located in a super safe '\n",
      " 'neighbourhood and close to super markets banks shopping mall and several '\n",
      " 'restaurants 3 bedrooms 5 beds 2 bathrooms a large kitchen 1 parking you will '\n",
      " \"be impressed by its large living room in the basement it's terrace and \"\n",
      " 'private garden',\n",
      " 'charming apartment in the pr√©fontaine ontario district of montreal for 6 '\n",
      " 'people ideal for families friends or business travelers',\n",
      " \"you won't find a better more comfortable montreal experience than this our \"\n",
      " 'home in outremont is bright and spacious with a newly renovated kitchen and '\n",
      " 'bathroom on the corner bernard three blocks from parc avenue and mile end '\n",
      " 'best spot in town',\n",
      " 'discover montreal from an unique and intimate space located within the old '\n",
      " \"montreal the loft is from walking distance of all the city's most popular \"\n",
      " 'attractions']\n"
     ]
    }
   ],
   "source": [
    "# Prints the padded sequences\n",
    "pprint(tokenizer.sequences_to_texts(x_tr_text_id[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "75tmwBWAR1Zm",
    "outputId": "b8112f5f-d12e-4ec2-f779-04cb38850d5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words in the dictionary: 40000\n"
     ]
    }
   ],
   "source": [
    "# Prints the total words in vocabulary\n",
    "print('total words in the dictionary:', tokenizer.num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports required libraries\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D\n",
    "from tensorflow.keras.layers import Bidirectional, GRU, LSTM, Attention, MultiHeadAttention, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üåà Approach 1: Multi Objective (Predicting Price and Type) Multi Modality (Image + Text) Price Prediction\n",
    "##### Here both text and image inputs are considered for the training and both prices and types for the test dataset are predicted/classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 64, 64, 2)]  0           []                               \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 49, 49, 32)   16416       ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 100, 100)     4000000     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 3, 3, 32)     0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean (TFOpLambd  (None, 100)         0           ['embedding[0][0]']              \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 288)          0           ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " tf.concat (TFOpLambda)         (None, 388)          0           ['tf.math.reduce_mean[0][0]',    \n",
      "                                                                  'flatten[0][0]']                \n",
      "                                                                                                  \n",
      " price (Dense)                  (None, 3)            1167        ['tf.concat[0][0]']              \n",
      "                                                                                                  \n",
      " type (Dense)                   (None, 24)           9336        ['tf.concat[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,026,919\n",
      "Trainable params: 4,026,919\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Input layer for text and image inputs\n",
    "in_text = keras.Input(batch_shape=(None, max_len))\n",
    "in_image = keras.Input(batch_shape=(None, 64, 64, 2))\n",
    "\n",
    "# text part\n",
    "# Embedding layer for the text inputs\n",
    "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)\n",
    "# Apply reduce mean to reduce the shape of the tensor for all inputs\n",
    "averaged = tf.reduce_mean(embedded, axis=1)\n",
    "\n",
    "\n",
    "# image part\n",
    "# Convolutional layer taking image inputs\n",
    "cov = Conv2D(32, (16, 16))(in_image)\n",
    "# Max Pooling layer taking output from the convolutional layer as it's input\n",
    "pl = MaxPool2D((16, 16))(cov)\n",
    "# Flatten layer that flattens the images \n",
    "flattened = Flatten()(pl)\n",
    "\n",
    "\n",
    "# fusion:\n",
    "# Concatenates the layers for both text and image parts\n",
    "fused = tf.concat([averaged, flattened], axis=-1)\n",
    "\n",
    "# multi-objectives (each is a multi-class classification)\n",
    "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
    "p_type = Dense(len_type, activation='softmax', name='type')(fused)\n",
    "\n",
    "# Provides the inputs and outputs of the model\n",
    "model = keras.Model(\n",
    "    inputs={\n",
    "        'summary': in_text,\n",
    "        'image': in_image\n",
    "    },\n",
    "    outputs={\n",
    "        'price': p_price,\n",
    "        'type': p_type,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compiles the model and defines the loss functions, loss weights and evaluation metrics\n",
    "# to be utilised for each objective respectively\n",
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss={\n",
    "        'price': 'sparse_categorical_crossentropy',\n",
    "        'type': 'sparse_categorical_crossentropy',\n",
    "    },\n",
    "    loss_weights={\n",
    "        'price': 0.5,\n",
    "        'type': 0.5,       \n",
    "    },\n",
    "    metrics={\n",
    "        'price': ['SparseCategoricalAccuracy'],\n",
    "        'type': ['SparseCategoricalAccuracy'],\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "305/305 [==============================] - 20s 63ms/step - loss: 25.5367 - price_loss: 24.0028 - type_loss: 27.0707 - price_sparse_categorical_accuracy: 0.4879 - type_sparse_categorical_accuracy: 0.5752 - val_loss: 14.2379 - val_price_loss: 10.1294 - val_type_loss: 18.3464 - val_price_sparse_categorical_accuracy: 0.6011 - val_type_sparse_categorical_accuracy: 0.7469\n",
      "Epoch 2/20\n",
      "305/305 [==============================] - 19s 63ms/step - loss: 8.6141 - price_loss: 5.4652 - type_loss: 11.7630 - price_sparse_categorical_accuracy: 0.5078 - type_sparse_categorical_accuracy: 0.5773 - val_loss: 5.1708 - val_price_loss: 4.1599 - val_type_loss: 6.1817 - val_price_sparse_categorical_accuracy: 0.4054 - val_type_sparse_categorical_accuracy: 0.5455\n",
      "Epoch 3/20\n",
      "305/305 [==============================] - 21s 68ms/step - loss: 6.2286 - price_loss: 4.5061 - type_loss: 7.9510 - price_sparse_categorical_accuracy: 0.5287 - type_sparse_categorical_accuracy: 0.5936 - val_loss: 6.6674 - val_price_loss: 4.4687 - val_type_loss: 8.8660 - val_price_sparse_categorical_accuracy: 0.4595 - val_type_sparse_categorical_accuracy: 0.5397\n",
      "Epoch 4/20\n",
      "305/305 [==============================] - 19s 64ms/step - loss: 11.9107 - price_loss: 7.7774 - type_loss: 16.0440 - price_sparse_categorical_accuracy: 0.5314 - type_sparse_categorical_accuracy: 0.5840 - val_loss: 10.1031 - val_price_loss: 8.4632 - val_type_loss: 11.7431 - val_price_sparse_categorical_accuracy: 0.5463 - val_type_sparse_categorical_accuracy: 0.6765\n",
      "Epoch 5/20\n",
      "305/305 [==============================] - 19s 64ms/step - loss: 9.8354 - price_loss: 6.8555 - type_loss: 12.8154 - price_sparse_categorical_accuracy: 0.5525 - type_sparse_categorical_accuracy: 0.5914 - val_loss: 12.5568 - val_price_loss: 7.3581 - val_type_loss: 17.7555 - val_price_sparse_categorical_accuracy: 0.3841 - val_type_sparse_categorical_accuracy: 0.6143\n",
      "Epoch 6/20\n",
      "305/305 [==============================] - 20s 64ms/step - loss: 8.6679 - price_loss: 5.5025 - type_loss: 11.8333 - price_sparse_categorical_accuracy: 0.5633 - type_sparse_categorical_accuracy: 0.6068 - val_loss: 14.2322 - val_price_loss: 7.8277 - val_type_loss: 20.6367 - val_price_sparse_categorical_accuracy: 0.5913 - val_type_sparse_categorical_accuracy: 0.7387\n",
      "Epoch 7/20\n",
      "305/305 [==============================] - 20s 65ms/step - loss: 10.4506 - price_loss: 8.6405 - type_loss: 12.2607 - price_sparse_categorical_accuracy: 0.5654 - type_sparse_categorical_accuracy: 0.6018 - val_loss: 17.2706 - val_price_loss: 10.9141 - val_type_loss: 23.6272 - val_price_sparse_categorical_accuracy: 0.6167 - val_type_sparse_categorical_accuracy: 0.7002\n"
     ]
    }
   ],
   "source": [
    "# Train the model by providing respecitve inputs, outputs and hyper-parameters\n",
    "history = model.fit(\n",
    "    x={\n",
    "        'summary': x_tr_text_id,\n",
    "        'image': x_tr_image\n",
    "    },\n",
    "    y={\n",
    "        'price': y_tr_price,\n",
    "        'type': y_tr_type,\n",
    "    },\n",
    "    epochs=20,\n",
    "    batch_size=16,\n",
    "    validation_data=(\n",
    "        {\n",
    "            'summary': x_vl_text_id,\n",
    "            'image': x_vl_image\n",
    "         }, \n",
    "        {\n",
    "            'price': y_vl_price,\n",
    "            'type': y_vl_type,\n",
    "        }),\n",
    "    validation_split=0.2,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=5, )\n",
    "    ],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a93ddc0f8dd41b99c5a70937c853075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and preprocess test dataset \n",
    "x_test_summary = _preprocess(clean_text(x_test_df.summary.astype(str)))\n",
    "x_test_image = np.array([load_image(i) for i in tqdm(x_test_df.image)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0000000e+00 3.3960576e-13 3.8120925e-27]\n",
      " [1.0000000e+00 2.2703590e-09 3.3316945e-20]\n",
      " [1.0000000e+00 1.0824574e-10 6.9895137e-25]\n",
      " ...\n",
      " [9.9999988e-01 8.5905363e-08 2.0315296e-24]\n",
      " [1.0000000e+00 4.0475201e-10 5.0402993e-31]\n",
      " [1.0000000e+00 1.6841716e-09 1.4552738e-31]]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Predict the outputs for the test dataset\n",
    "y_predict = model.predict(\n",
    "    {\n",
    "        'summary': x_test_summary,\n",
    "        'image': x_test_image\n",
    "    }\n",
    ")\n",
    "\n",
    "# Process the model outputs\n",
    "price_predicted = y_predict['price']\n",
    "print(price_predicted)\n",
    "price_category_predicted = np.argmax(price_predicted, axis=1)\n",
    "print(price_category_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0000000e+00 9.9999988e-01 0.0000000e+00 ... 0.0000000e+00\n",
      "  9.8215414e-19 0.0000000e+00]\n",
      " [0.0000000e+00 9.9999940e-01 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 1.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  3.9972043e-34 0.0000000e+00]\n",
      " ...\n",
      " [0.0000000e+00 1.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  5.6807624e-26 0.0000000e+00]\n",
      " [0.0000000e+00 2.6611995e-06 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 1.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  3.7401382e-37 0.0000000e+00]]\n",
      "[ 1  1  1 ...  1 17  1]\n"
     ]
    }
   ],
   "source": [
    "type_predicted = y_predict['type']\n",
    "print(type_predicted)\n",
    "type_category_predicted = np.argmax(type_predicted, axis=1)\n",
    "print(type_category_predicted)\n",
    "\n",
    "pd.DataFrame(\n",
    "    {'id': x_test_df.id,\n",
    "     'price': price_category_predicted,\n",
    "     'type': type_category_predicted}).to_csv('sample_submission_multi_objective.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üåà Approach 2: Multi Modality (Image + Text) Price Prediction\n",
    "##### Here both text and image inputs are considered for the training and price for the test dataset is predicted/classified.\n",
    "##### Tried following other hyper-parameter combinations as follows in the current implementation for this approach:\n",
    "1. Bidirectional GRU(32)\n",
    "2. Bidirectional GRU(64)\n",
    "3. More Dense Layers for image part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "          \n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = tf.nn.tanh(\n",
    "            self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "          \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_QkkRmqYR1Zm",
    "outputId": "f7bc4efb-5993-4507-ecec-69f4f8eb6389"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 100, 100)     4000000     ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " bi_lstm_0 (Bidirectional)      (None, 100, 256)     234496      ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 64, 64, 2)]  0           []                               \n",
      "                                                                                                  \n",
      " bi_lstm_1 (Bidirectional)      [(None, 100, 256),   394240      ['bi_lstm_0[0][0]']              \n",
      "                                 (None, 128),                                                     \n",
      "                                 (None, 128),                                                     \n",
      "                                 (None, 128),                                                     \n",
      "                                 (None, 128)]                                                     \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 49, 49, 32)   16416       ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 256)          0           ['bi_lstm_1[0][1]',              \n",
      "                                                                  'bi_lstm_1[0][3]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 3, 3, 32)    0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " attention (Attention)          ((None, 256),        5151        ['bi_lstm_1[0][0]',              \n",
      "                                 (None, 100, 1))                  'concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 288)          0           ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 16)           4112        ['attention[0][0]']              \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 16)           4624        ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 16)           0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 16)           0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " tf.concat_1 (TFOpLambda)       (None, 32)           0           ['dropout[0][0]',                \n",
      "                                                                  'dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " price (Dense)                  (None, 3)            99          ['tf.concat_1[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,659,138\n",
      "Trainable params: 4,659,138\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# in_text = keras.Input(batch_shape=(None, max_len))\n",
    "in_image = keras.Input(batch_shape=(None, 64, 64, 2))\n",
    "\n",
    "# text part\n",
    "# embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)\n",
    "# z = Bidirectional(GRU(64))(embedded)\n",
    "# averaged = tf.reduce_mean(z, axis=1)\n",
    "embed_size = 100\n",
    "sequence_input = keras.Input(shape=(max_len,), dtype=\"int32\")\n",
    "embedded_sequences = keras.layers.Embedding(vocab_size, embed_size)(sequence_input)\n",
    "\n",
    "rnn_cell_size = 128\n",
    "lstm = Bidirectional(LSTM(rnn_cell_size, return_sequences = True), name=\"bi_lstm_0\")(embedded_sequences)\n",
    "# Getting our LSTM outputs\n",
    "(lstm, forward_h, forward_c, backward_h, backward_c) = Bidirectional(LSTM(rnn_cell_size, return_sequences=True, return_state=True), name=\"bi_lstm_1\")(lstm)\n",
    "state_h = keras.layers.Concatenate()([forward_h, backward_h])\n",
    "state_c = keras.layers.Concatenate()([forward_c, backward_c])\n",
    "context_vector, attention_weights = Attention(10)(lstm, state_h)\n",
    "dense1 = Dense(16, activation=\"relu\")(context_vector)\n",
    "dropout_text = Dropout(0.3)(dense1)\n",
    "\n",
    "# image part\n",
    "cov = Conv2D(32, (16, 16))(in_image)\n",
    "\n",
    "pl = MaxPool2D((16, 16))(cov)\n",
    "flattened = Flatten()(pl)\n",
    "dense = Dense(16)(flattened)\n",
    "dropout = keras.layers.Dropout(0.3)(dense)\n",
    "\n",
    "\n",
    "# fusion:\n",
    "# fused = tf.concat([averaged, flattened], axis=-1)\n",
    "# fused = tf.concat([z, dropout], axis=-1)\n",
    "fused = tf.concat([dropout_text, dropout], axis=-1)\n",
    "\n",
    "# multi-objectives (each is a multi-class classification)\n",
    "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
    "# p_type = Dense(len_type, activation='softmax', name='type')(fused)\n",
    "\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs={\n",
    "        'summary': sequence_input,\n",
    "        'image': in_image\n",
    "    },\n",
    "    outputs={\n",
    "        'price': p_price,\n",
    "#         'type': p_type,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss={\n",
    "        'price': 'sparse_categorical_crossentropy',\n",
    "#         'type': 'sparse_categorical_crossentropy',\n",
    "    },\n",
    "    loss_weights={\n",
    "        'price': 0.5,\n",
    "#         'type': 0.5,       \n",
    "    },\n",
    "    metrics={\n",
    "        'price': ['SparseCategoricalAccuracy'],\n",
    "#         'type': ['SparseCategoricalAccuracy'],\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lyVFF4LTR1Zm",
    "outputId": "66daad91-8ddd-48f7-95c2-69acd9c838d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "305/305 [==============================] - 55s 167ms/step - loss: 18.4906 - sparse_categorical_accuracy: 0.4814 - val_loss: 3.8401 - val_sparse_categorical_accuracy: 0.4955\n",
      "Epoch 2/50\n",
      "305/305 [==============================] - 50s 165ms/step - loss: 5.8784 - sparse_categorical_accuracy: 0.4977 - val_loss: 2.9820 - val_sparse_categorical_accuracy: 0.6077\n",
      "Epoch 3/50\n",
      "305/305 [==============================] - 51s 167ms/step - loss: 2.7297 - sparse_categorical_accuracy: 0.5025 - val_loss: 1.4534 - val_sparse_categorical_accuracy: 0.5512\n",
      "Epoch 4/50\n",
      "305/305 [==============================] - 58s 191ms/step - loss: 1.6079 - sparse_categorical_accuracy: 0.5141 - val_loss: 0.9994 - val_sparse_categorical_accuracy: 0.4701\n",
      "Epoch 5/50\n",
      "305/305 [==============================] - 56s 184ms/step - loss: 1.0010 - sparse_categorical_accuracy: 0.5480 - val_loss: 0.7739 - val_sparse_categorical_accuracy: 0.5676\n",
      "Epoch 6/50\n",
      "305/305 [==============================] - 55s 181ms/step - loss: 0.7709 - sparse_categorical_accuracy: 0.5580 - val_loss: 0.5763 - val_sparse_categorical_accuracy: 0.5577\n",
      "Epoch 7/50\n",
      "305/305 [==============================] - 56s 182ms/step - loss: 0.5527 - sparse_categorical_accuracy: 0.5926 - val_loss: 0.5238 - val_sparse_categorical_accuracy: 0.5520\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x={\n",
    "        'summary': x_tr_text_id,\n",
    "        'image': x_tr_image\n",
    "    },\n",
    "    y={\n",
    "        'price': y_tr_price,\n",
    "#         'type': y_tr_type,\n",
    "    },\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_data=(\n",
    "        {\n",
    "            'summary': x_vl_text_id,\n",
    "            'image': x_vl_image\n",
    "         }, \n",
    "        {\n",
    "            'price': y_vl_price,\n",
    "#             'type': y_vl_type,\n",
    "        }),\n",
    "    validation_split=0.2,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_sparse_categorical_accuracy', patience=5, )\n",
    "    ],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6df640ace73450898377f4f26e61031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_test_summary = _preprocess(clean_text(x_test_df.summary.astype(str)))\n",
    "x_test_image = np.array([load_image(i) for i in tqdm(x_test_df.image)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'price': array([[6.0268295e-01, 3.8016829e-01, 1.7148769e-02],\n",
       "        [9.0975350e-01, 9.0031676e-02, 2.1480401e-04],\n",
       "        [6.8708915e-01, 9.6717514e-02, 2.1619333e-01],\n",
       "        ...,\n",
       "        [8.7592030e-01, 1.2398951e-01, 9.0192800e-05],\n",
       "        [6.6669488e-01, 3.1563118e-01, 1.7673977e-02],\n",
       "        [3.4807354e-01, 6.4041358e-01, 1.1512849e-02]], dtype=float32)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = model.predict(\n",
    "    {\n",
    "        'summary': x_test_summary,\n",
    "        'image': x_test_image\n",
    "    }\n",
    ")\n",
    "\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.0268295e-01 3.8016829e-01 1.7148769e-02]\n",
      " [9.0975350e-01 9.0031676e-02 2.1480401e-04]\n",
      " [6.8708915e-01 9.6717514e-02 2.1619333e-01]\n",
      " ...\n",
      " [8.7592030e-01 1.2398951e-01 9.0192800e-05]\n",
      " [6.6669488e-01 3.1563118e-01 1.7673977e-02]\n",
      " [3.4807354e-01 6.4041358e-01 1.1512849e-02]]\n",
      "[0 0 0 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "price_predicted = y_predict['price']\n",
    "print(price_predicted)\n",
    "price_category_predicted = np.argmax(price_predicted, axis=1)\n",
    "print(price_category_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    {'id': x_test_df.id,\n",
    "     'price': price_category_predicted}).to_csv('sample_submission_multi_modality.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üåà Approach 3: Text inputs with BiDirectional GRU/LSTM layer (Multiobjective included)\n",
    "##### Here only text inputs are considered for the training using Bidirectional GRU and Bidirectional LSTM and price for the test dataset is predicted/classified. Additionally type of the listings can also be predicted by uncommenting the commented code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 100)]             0         \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, 100, 100)          4000000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 128)              63744     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " price (Dense)               (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,064,131\n",
      "Trainable params: 4,064,131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "in_text = keras.Input(batch_shape=(None, max_len))\n",
    "\n",
    "# text part\n",
    "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)\n",
    "# z = Bidirectional(LSTM(32))(embedded)\n",
    "z = Bidirectional(GRU(64))(embedded)\n",
    "\n",
    "# multi-objectives (each is a multi-class classification)\n",
    "p_price = Dense(len_price, activation='softmax', name='price')(z)\n",
    "# p_type = Dense(len_type, activation='softmax', name='type')(z)\n",
    "\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs={\n",
    "        'summary': in_text\n",
    "    },\n",
    "    outputs={\n",
    "        'price': p_price,\n",
    "#         'type': p_type,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss={\n",
    "        'price': 'sparse_categorical_crossentropy',\n",
    "#         'type': 'sparse_categorical_crossentropy',\n",
    "    },\n",
    "    loss_weights={\n",
    "        'price': 0.5,\n",
    "#         'type': 0.5,       \n",
    "    },\n",
    "    metrics={\n",
    "        'price': ['SparseCategoricalAccuracy'],\n",
    "#         'type': ['SparseCategoricalAccuracy'],\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "305/305 [==============================] - 18s 54ms/step - loss: 0.3995 - sparse_categorical_accuracy: 0.6381 - val_loss: 0.3849 - val_sparse_categorical_accuracy: 0.6560\n",
      "Epoch 2/20\n",
      "305/305 [==============================] - 17s 56ms/step - loss: 0.3338 - sparse_categorical_accuracy: 0.7064 - val_loss: 0.3849 - val_sparse_categorical_accuracy: 0.6511\n",
      "Epoch 3/20\n",
      "305/305 [==============================] - 17s 57ms/step - loss: 0.2721 - sparse_categorical_accuracy: 0.7750 - val_loss: 0.4117 - val_sparse_categorical_accuracy: 0.6306\n",
      "Epoch 4/20\n",
      "305/305 [==============================] - 17s 57ms/step - loss: 0.2179 - sparse_categorical_accuracy: 0.8189 - val_loss: 0.4551 - val_sparse_categorical_accuracy: 0.6568\n",
      "Epoch 5/20\n",
      "305/305 [==============================] - 18s 58ms/step - loss: 0.1623 - sparse_categorical_accuracy: 0.8727 - val_loss: 0.5256 - val_sparse_categorical_accuracy: 0.6143\n",
      "Epoch 6/20\n",
      "305/305 [==============================] - 15s 51ms/step - loss: 0.1308 - sparse_categorical_accuracy: 0.8971 - val_loss: 0.6281 - val_sparse_categorical_accuracy: 0.6093\n",
      "Epoch 7/20\n",
      "305/305 [==============================] - 16s 53ms/step - loss: 0.1014 - sparse_categorical_accuracy: 0.9215 - val_loss: 0.6747 - val_sparse_categorical_accuracy: 0.6069\n",
      "Epoch 8/20\n",
      "305/305 [==============================] - 16s 53ms/step - loss: 0.0919 - sparse_categorical_accuracy: 0.9258 - val_loss: 0.7352 - val_sparse_categorical_accuracy: 0.6372\n",
      "Epoch 9/20\n",
      "305/305 [==============================] - 14s 47ms/step - loss: 0.0786 - sparse_categorical_accuracy: 0.9389 - val_loss: 0.7539 - val_sparse_categorical_accuracy: 0.6314\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x={\n",
    "        'summary': x_tr_text_id\n",
    "    },\n",
    "    y={\n",
    "        'price': y_tr_price,\n",
    "#         'type': y_tr_type,\n",
    "    },\n",
    "    epochs=20,\n",
    "    batch_size=16,\n",
    "    validation_data=(\n",
    "        {\n",
    "            'summary': x_vl_text_id\n",
    "         }, \n",
    "        {\n",
    "            'price': y_vl_price,\n",
    "#             'type': y_vl_type,\n",
    "        }),\n",
    "    validation_split=0.2,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_sparse_categorical_accuracy', patience=5, )\n",
    "    ],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_summary = _preprocess(clean_text(x_test_df.summary.astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'price': array([[1.7677167e-04, 9.9077034e-01, 9.0528112e-03],\n",
       "        [9.9996281e-01, 3.4082128e-05, 3.0863500e-06],\n",
       "        [9.9993992e-01, 5.6576293e-05, 3.4882057e-06],\n",
       "        ...,\n",
       "        [1.4041792e-01, 8.4968174e-01, 9.9002579e-03],\n",
       "        [9.9998450e-01, 1.3129831e-05, 2.3274135e-06],\n",
       "        [3.3826292e-02, 5.6275493e-01, 4.0341881e-01]], dtype=float32)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = model.predict(\n",
    "    {\n",
    "        'summary': x_test_summary\n",
    "    }\n",
    ")\n",
    "\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.7677167e-04 9.9077034e-01 9.0528112e-03]\n",
      " [9.9996281e-01 3.4082128e-05 3.0863500e-06]\n",
      " [9.9993992e-01 5.6576293e-05 3.4882057e-06]\n",
      " ...\n",
      " [1.4041792e-01 8.4968174e-01 9.9002579e-03]\n",
      " [9.9998450e-01 1.3129831e-05 2.3274135e-06]\n",
      " [3.3826292e-02 5.6275493e-01 4.0341881e-01]]\n",
      "[1 0 0 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "price_predicted = y_predict['price']\n",
    "print(price_predicted)\n",
    "price_category_predicted = np.argmax(price_predicted, axis=1)\n",
    "print(price_category_predicted)\n",
    "\n",
    "pd.DataFrame(\n",
    "    {'id': x_test_df.id,\n",
    "     'price': price_category_predicted}).to_csv('sample_submission_text_inputs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üåà Approach 4: Text inputs with Attention layer (Multiobjective included)\n",
    "##### Here only text inputs are considered for the training using Attention and price for the test dataset is predicted/classified. Additionally type of the listings can also be predicted by uncommenting the commented code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "          \n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = tf.nn.tanh(\n",
    "            self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "          \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 100, 100)     4000000     ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " bi_lstm_0 (Bidirectional)      (None, 100, 256)     234496      ['embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " bi_lstm_1 (Bidirectional)      [(None, 100, 256),   394240      ['bi_lstm_0[0][0]']              \n",
      "                                 (None, 128),                                                     \n",
      "                                 (None, 128),                                                     \n",
      "                                 (None, 128),                                                     \n",
      "                                 (None, 128)]                                                     \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 256)          0           ['bi_lstm_1[0][1]',              \n",
      "                                                                  'bi_lstm_1[0][3]']              \n",
      "                                                                                                  \n",
      " attention_1 (Attention)        ((None, 256),        5151        ['bi_lstm_1[0][0]',              \n",
      "                                 (None, 100, 1))                  'concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 16)           4112        ['attention_1[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 16)           0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " price (Dense)                  (None, 3)            51          ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,638,050\n",
      "Trainable params: 4,638,050\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_size = 100\n",
    "sequence_input = keras.Input(shape=(max_len,), dtype=\"int32\")\n",
    "embedded_sequences = keras.layers.Embedding(vocab_size, embed_size)(sequence_input)\n",
    "\n",
    "rnn_cell_size = 128\n",
    "lstm = Bidirectional(LSTM(rnn_cell_size, return_sequences = True), name=\"bi_lstm_0\")(embedded_sequences)\n",
    "# Getting our LSTM outputs\n",
    "(lstm, forward_h, forward_c, backward_h, backward_c) = Bidirectional(LSTM(rnn_cell_size, return_sequences=True, return_state=True), name=\"bi_lstm_1\")(lstm)\n",
    "state_h = keras.layers.Concatenate()([forward_h, backward_h])\n",
    "state_c = keras.layers.Concatenate()([forward_c, backward_c])\n",
    "context_vector, attention_weights = Attention(10)(lstm, state_h)\n",
    "dense1 = Dense(16, activation=\"relu\")(context_vector)\n",
    "dropout = Dropout(0.3)(dense1)\n",
    "p_price = Dense(len_price, activation='softmax', name='price')(dropout)\n",
    "\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs={\n",
    "        'summary': sequence_input\n",
    "    },\n",
    "    outputs={\n",
    "        'price': p_price,\n",
    "#         'type': p_type,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss={\n",
    "        'price': 'sparse_categorical_crossentropy',\n",
    "#         'type': 'sparse_categorical_crossentropy',\n",
    "    },\n",
    "    loss_weights={\n",
    "        'price': 0.5,\n",
    "#         'type': 0.5,       \n",
    "    },\n",
    "    metrics={\n",
    "        'price': ['SparseCategoricalAccuracy'],\n",
    "#         'type': ['SparseCategoricalAccuracy'],\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "305/305 [==============================] - 47s 143ms/step - loss: 0.4509 - sparse_categorical_accuracy: 0.5494 - val_loss: 0.3951 - val_sparse_categorical_accuracy: 0.6183\n",
      "Epoch 2/50\n",
      "305/305 [==============================] - 46s 150ms/step - loss: 0.3878 - sparse_categorical_accuracy: 0.6668 - val_loss: 0.3774 - val_sparse_categorical_accuracy: 0.6429\n",
      "Epoch 3/50\n",
      "305/305 [==============================] - 46s 151ms/step - loss: 0.3417 - sparse_categorical_accuracy: 0.7227 - val_loss: 0.3847 - val_sparse_categorical_accuracy: 0.6470\n",
      "Epoch 4/50\n",
      "305/305 [==============================] - 41s 134ms/step - loss: 0.3042 - sparse_categorical_accuracy: 0.7572 - val_loss: 0.4269 - val_sparse_categorical_accuracy: 0.6503\n",
      "Epoch 5/50\n",
      "305/305 [==============================] - 41s 135ms/step - loss: 0.2760 - sparse_categorical_accuracy: 0.7857 - val_loss: 0.4222 - val_sparse_categorical_accuracy: 0.6143\n",
      "Epoch 6/50\n",
      "305/305 [==============================] - 44s 144ms/step - loss: 0.2385 - sparse_categorical_accuracy: 0.8178 - val_loss: 0.4847 - val_sparse_categorical_accuracy: 0.6544\n",
      "Epoch 7/50\n",
      "305/305 [==============================] - 42s 138ms/step - loss: 0.2076 - sparse_categorical_accuracy: 0.8504 - val_loss: 0.5793 - val_sparse_categorical_accuracy: 0.6331\n",
      "Epoch 8/50\n",
      "305/305 [==============================] - 43s 141ms/step - loss: 0.1796 - sparse_categorical_accuracy: 0.8750 - val_loss: 0.6059 - val_sparse_categorical_accuracy: 0.6396\n",
      "Epoch 9/50\n",
      "305/305 [==============================] - 42s 139ms/step - loss: 0.1489 - sparse_categorical_accuracy: 0.8939 - val_loss: 0.7762 - val_sparse_categorical_accuracy: 0.6429\n",
      "Epoch 10/50\n",
      "305/305 [==============================] - 52s 171ms/step - loss: 0.1254 - sparse_categorical_accuracy: 0.9111 - val_loss: 0.9031 - val_sparse_categorical_accuracy: 0.6511\n",
      "Epoch 11/50\n",
      "305/305 [==============================] - 58s 191ms/step - loss: 0.1087 - sparse_categorical_accuracy: 0.9162 - val_loss: 0.7538 - val_sparse_categorical_accuracy: 0.6429\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x={\n",
    "        'summary': x_tr_text_id\n",
    "    },\n",
    "    y={\n",
    "        'price': y_tr_price,\n",
    "#         'type': y_tr_type,\n",
    "    },\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_data=(\n",
    "        {\n",
    "            'summary': x_vl_text_id\n",
    "         }, \n",
    "        {\n",
    "            'price': y_vl_price,\n",
    "#             'type': y_vl_type,\n",
    "        }),\n",
    "    validation_split=0.2,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_sparse_categorical_accuracy', patience=5, )\n",
    "    ],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "446e1b59e04b4a90929b6e6f2718be12",
      "805c9e1efef243fc87c30cc23f9df6f8",
      "ea09f53200bf4e35ac88b755147fc337",
      "a3e3aebc1d194f5f9b1680639e9e072c",
      "1df51aaafc214051b459f8fe3c828593",
      "eec96bdbe3354f5b8234c5362bc6838a",
      "5ae3e86ecd1f4d0f9b3cbf700ab11c7a",
      "1c2c8a347f3442a28d07c41c8c84dd27",
      "e3c14506bf70453289ac73a468d40798"
     ]
    },
    "id": "6fxoXMDER1Zm",
    "outputId": "7ae7b542-eafc-4fb1-d536-b5b477de1bca"
   },
   "outputs": [],
   "source": [
    "# x_test_summary = _preprocess(clean_text(x_test_df.summary.astype(str)))\n",
    "x_test_summary = _preprocess(x_test_df.summary.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'price': array([[4.6335861e-01, 4.2944407e-01, 1.0719736e-01],\n",
       "        [9.9999738e-01, 2.6633322e-06, 3.7061049e-08],\n",
       "        [9.9999440e-01, 5.5388832e-06, 8.6566928e-08],\n",
       "        ...,\n",
       "        [5.9235317e-01, 3.3369261e-01, 7.3954217e-02],\n",
       "        [9.9999988e-01, 6.8871188e-08, 4.3052434e-10],\n",
       "        [5.5936056e-01, 3.5308519e-01, 8.7554179e-02]], dtype=float32)}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = model.predict(\n",
    "    {\n",
    "        'summary': x_test_summary\n",
    "    }\n",
    ")\n",
    "\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.6335861e-01 4.2944407e-01 1.0719736e-01]\n",
      " [9.9999738e-01 2.6633322e-06 3.7061049e-08]\n",
      " [9.9999440e-01 5.5388832e-06 8.6566928e-08]\n",
      " ...\n",
      " [5.9235317e-01 3.3369261e-01 7.3954217e-02]\n",
      " [9.9999988e-01 6.8871188e-08 4.3052434e-10]\n",
      " [5.5936056e-01 3.5308519e-01 8.7554179e-02]]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "price_predicted = y_predict['price']\n",
    "print(price_predicted)\n",
    "price_category_predicted = np.argmax(price_predicted, axis=1)\n",
    "print(price_category_predicted)\n",
    "\n",
    "pd.DataFrame(\n",
    "    {'id': x_test_df.id,\n",
    "     'price': price_category_predicted}).to_csv('sample_submission_text_inputs_attention.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üåà Approach 5: Image inputs with Conv2D + Dropout layer (Multiobjective included)\n",
    "##### Here only image inputs are considered for the training using Convolutional and Dropout layers and price for the test dataset is predicted/classified. Additionally type of the listings can also be predicted by uncommenting the commented code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 64, 64, 2)]       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 49, 49, 32)        16416     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 3, 3, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 288)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 32)                9248      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " price (Dense)               (None, 3)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26,243\n",
      "Trainable params: 26,243\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "in_image = keras.Input(batch_shape=(None, 64, 64, 2))\n",
    "\n",
    "# image part\n",
    "cov = Conv2D(32, (16, 16))(in_image)\n",
    "pl = MaxPool2D((16, 16))(cov)\n",
    "flattened = Flatten()(pl)\n",
    "dense1 = Dense(32)(flattened)\n",
    "dropout1 = Dropout(0.3)(dense1)\n",
    "dense = Dense(16)(dropout1)\n",
    "dropout = Dropout(0.3)(dense)\n",
    "\n",
    "# multi-objectives (each is a multi-class classification)\n",
    "p_price = Dense(len_price, activation='softmax', name='price')(dropout)\n",
    "# p_type = Dense(len_type, activation='softmax', name='type')(flattened)\n",
    "\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs={\n",
    "        'image': in_image\n",
    "    },\n",
    "    outputs={\n",
    "        'price': p_price,\n",
    "#         'type': p_type,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss={\n",
    "        'price': 'sparse_categorical_crossentropy',\n",
    "#         'type': 'sparse_categorical_crossentropy',\n",
    "    },\n",
    "    loss_weights={\n",
    "        'price': 0.5,\n",
    "#         'type': 0.5,       \n",
    "    },\n",
    "    metrics={\n",
    "        'price': ['SparseCategoricalAccuracy'],\n",
    "#         'type': ['SparseCategoricalAccuracy'],\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "305/305 [==============================] - 18s 57ms/step - loss: 38.0771 - sparse_categorical_accuracy: 0.4641 - val_loss: 3.9312 - val_sparse_categorical_accuracy: 0.5184\n",
      "Epoch 2/20\n",
      "305/305 [==============================] - 16s 52ms/step - loss: 11.5060 - sparse_categorical_accuracy: 0.4770 - val_loss: 5.2026 - val_sparse_categorical_accuracy: 0.6183\n",
      "Epoch 3/20\n",
      "305/305 [==============================] - 16s 51ms/step - loss: 5.0771 - sparse_categorical_accuracy: 0.4768 - val_loss: 1.5055 - val_sparse_categorical_accuracy: 0.6077\n",
      "Epoch 4/20\n",
      "305/305 [==============================] - 16s 53ms/step - loss: 2.9318 - sparse_categorical_accuracy: 0.4799 - val_loss: 1.2794 - val_sparse_categorical_accuracy: 0.6110\n",
      "Epoch 5/20\n",
      "305/305 [==============================] - 17s 55ms/step - loss: 1.8617 - sparse_categorical_accuracy: 0.4916 - val_loss: 0.8389 - val_sparse_categorical_accuracy: 0.6028\n",
      "Epoch 6/20\n",
      "305/305 [==============================] - 17s 57ms/step - loss: 1.3700 - sparse_categorical_accuracy: 0.4924 - val_loss: 0.5584 - val_sparse_categorical_accuracy: 0.5930\n",
      "Epoch 7/20\n",
      "305/305 [==============================] - 17s 57ms/step - loss: 0.9681 - sparse_categorical_accuracy: 0.4998 - val_loss: 0.5422 - val_sparse_categorical_accuracy: 0.6151\n",
      "Epoch 8/20\n",
      "305/305 [==============================] - 18s 58ms/step - loss: 0.7840 - sparse_categorical_accuracy: 0.5078 - val_loss: 0.4399 - val_sparse_categorical_accuracy: 0.6061\n",
      "Epoch 9/20\n",
      "305/305 [==============================] - 18s 59ms/step - loss: 0.6150 - sparse_categorical_accuracy: 0.5152 - val_loss: 0.4445 - val_sparse_categorical_accuracy: 0.5971\n",
      "Epoch 10/20\n",
      "305/305 [==============================] - 18s 59ms/step - loss: 0.5244 - sparse_categorical_accuracy: 0.5473 - val_loss: 0.4394 - val_sparse_categorical_accuracy: 0.6192\n",
      "Epoch 11/20\n",
      "305/305 [==============================] - 16s 54ms/step - loss: 0.4827 - sparse_categorical_accuracy: 0.5676 - val_loss: 0.4305 - val_sparse_categorical_accuracy: 0.6183\n",
      "Epoch 12/20\n",
      "305/305 [==============================] - 16s 53ms/step - loss: 0.4472 - sparse_categorical_accuracy: 0.5914 - val_loss: 0.4270 - val_sparse_categorical_accuracy: 0.6183\n",
      "Epoch 13/20\n",
      "305/305 [==============================] - 16s 53ms/step - loss: 0.4275 - sparse_categorical_accuracy: 0.6111 - val_loss: 0.4172 - val_sparse_categorical_accuracy: 0.6183\n",
      "Epoch 14/20\n",
      "305/305 [==============================] - 17s 56ms/step - loss: 0.4187 - sparse_categorical_accuracy: 0.6197 - val_loss: 0.4180 - val_sparse_categorical_accuracy: 0.6183\n",
      "Epoch 15/20\n",
      "305/305 [==============================] - 18s 58ms/step - loss: 0.4141 - sparse_categorical_accuracy: 0.6184 - val_loss: 0.4199 - val_sparse_categorical_accuracy: 0.6151\n",
      "Epoch 16/20\n",
      "305/305 [==============================] - 16s 53ms/step - loss: 0.4136 - sparse_categorical_accuracy: 0.6154 - val_loss: 0.4256 - val_sparse_categorical_accuracy: 0.6175\n",
      "Epoch 17/20\n",
      "305/305 [==============================] - 16s 52ms/step - loss: 0.4108 - sparse_categorical_accuracy: 0.6211 - val_loss: 0.4189 - val_sparse_categorical_accuracy: 0.6183\n",
      "Epoch 18/20\n",
      "305/305 [==============================] - 16s 51ms/step - loss: 0.4083 - sparse_categorical_accuracy: 0.6174 - val_loss: 0.4138 - val_sparse_categorical_accuracy: 0.6200\n",
      "Epoch 19/20\n",
      "305/305 [==============================] - 16s 51ms/step - loss: 0.4093 - sparse_categorical_accuracy: 0.6180 - val_loss: 0.4195 - val_sparse_categorical_accuracy: 0.6159\n",
      "Epoch 20/20\n",
      "305/305 [==============================] - 16s 53ms/step - loss: 0.4092 - sparse_categorical_accuracy: 0.6162 - val_loss: 0.4228 - val_sparse_categorical_accuracy: 0.6093\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x={\n",
    "        'image': x_tr_image\n",
    "    },\n",
    "    y={\n",
    "        'price': y_tr_price,\n",
    "#         'type': y_tr_type,\n",
    "    },\n",
    "    epochs=20,\n",
    "    batch_size=16,\n",
    "    validation_data=(\n",
    "        {\n",
    "            'image': x_vl_image\n",
    "         }, \n",
    "        {\n",
    "            'price': y_vl_price,\n",
    "#             'type': y_vl_type,\n",
    "        }),\n",
    "    validation_split=0.2,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, )\n",
    "    ],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a53a1b71841a4630a03d7e7ea6a33ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_test_image = np.array([load_image(i) for i in tqdm(x_test_df.image)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6177685 , 0.3175648 , 0.06466673],\n",
       "       [0.67025286, 0.18046376, 0.14928335],\n",
       "       [0.5668581 , 0.36610892, 0.06703304],\n",
       "       ...,\n",
       "       [0.5344046 , 0.31403083, 0.15156458],\n",
       "       [0.61348426, 0.3405027 , 0.04601301],\n",
       "       [0.62709963, 0.28782076, 0.08507963]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = model.predict(\n",
    "    {\n",
    "        'image': x_test_image\n",
    "    }\n",
    ")\n",
    "\n",
    "y_predict['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6177685  0.3175648  0.06466673]\n",
      " [0.67025286 0.18046376 0.14928335]\n",
      " [0.5668581  0.36610892 0.06703304]\n",
      " ...\n",
      " [0.5344046  0.31403083 0.15156458]\n",
      " [0.61348426 0.3405027  0.04601301]\n",
      " [0.62709963 0.28782076 0.08507963]]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "price_predicted = y_predict['price']\n",
    "print(price_predicted)\n",
    "price_category_predicted = np.argmax(price_predicted, axis=1)\n",
    "print(price_category_predicted)\n",
    "\n",
    "pd.DataFrame(\n",
    "    {'id': x_test_df.id,\n",
    "     'price': price_category_predicted}).to_csv('sample_submission_image_inputs.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05e4f892aa7043299e8675bf2329eb2c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "11e60d895e9142beab189eaf0fa0df14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6e11ee69f84d420a938e196b07ba6274",
       "IPY_MODEL_195c3403a30f4350a69abdc1caafa795"
      ],
      "layout": "IPY_MODEL_41e453fda22e4508afa28b421569e561"
     }
    },
    "148d0078a22543a395909db21f947d4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "195c3403a30f4350a69abdc1caafa795": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4aacc03bbd0b433a8ff3fbd8f1a72231",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_8354b303c3a14eec8ab2d81bd8a9da30",
      "value": " 7627/7627 [01:29&lt;00:00, 85.35it/s]"
     }
    },
    "1c2c8a347f3442a28d07c41c8c84dd27": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1df51aaafc214051b459f8fe3c828593": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "41e453fda22e4508afa28b421569e561": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "446e1b59e04b4a90929b6e6f2718be12": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ea09f53200bf4e35ac88b755147fc337",
       "IPY_MODEL_a3e3aebc1d194f5f9b1680639e9e072c"
      ],
      "layout": "IPY_MODEL_805c9e1efef243fc87c30cc23f9df6f8"
     }
    },
    "4aacc03bbd0b433a8ff3fbd8f1a72231": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ae3e86ecd1f4d0f9b3cbf700ab11c7a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6e11ee69f84d420a938e196b07ba6274": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_05e4f892aa7043299e8675bf2329eb2c",
      "max": 7627,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_148d0078a22543a395909db21f947d4d",
      "value": 7627
     }
    },
    "805c9e1efef243fc87c30cc23f9df6f8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8354b303c3a14eec8ab2d81bd8a9da30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a3e3aebc1d194f5f9b1680639e9e072c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c2c8a347f3442a28d07c41c8c84dd27",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_5ae3e86ecd1f4d0f9b3cbf700ab11c7a",
      "value": " 7360/7360 [01:14&lt;00:00, 99.30it/s]"
     }
    },
    "ea09f53200bf4e35ac88b755147fc337": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eec96bdbe3354f5b8234c5362bc6838a",
      "max": 7360,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1df51aaafc214051b459f8fe3c828593",
      "value": 7360
     }
    },
    "eec96bdbe3354f5b8234c5362bc6838a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
